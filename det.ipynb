{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-0879479a21ff>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-0879479a21ff>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    from ./examples/text/nms import nms\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import sys,time,math  \n",
    "caffe_root='./'  \n",
    "sys.path.insert(0, caffe_root + '/python')  \n",
    "import os,caffe\n",
    "from ./examples/text/nms import nms\n",
    "from ./examples/text/crop_image import crop_image\n",
    "\n",
    "#caffe.set_device(0)\n",
    "caffe.set_mode_cpu()\n",
    "\n",
    "\n",
    "def prepare_network(config):\n",
    "    net = caffe.Net(config['model_def'],\t # defines the structure of the model\n",
    "                config['model_weights'],  # contains the trained weights\n",
    "                caffe.TEST)     # use test mode (e.g., don't perform dropout)\n",
    "\n",
    "    transformer = caffe.io.Transformer({'data': (1,3,config['input_height'], config['input_width'])})\n",
    "    transformer.set_transpose('data', (2, 0, 1))\n",
    "    transformer.set_mean('data', np.array([104,117,123])) # mean pixel\n",
    "    transformer.set_raw_scale('data', 255)  # the reference model operates on images in [0,255] range instead of [0,1]\n",
    "    transformer.set_channel_swap('data', (2,1,0))  # the reference model has channels in BGR order instead of RGB\n",
    "\n",
    "    net.blobs['data'].reshape(1,3,config['input_height'], config['input_width'])\n",
    "\n",
    "    image=caffe.io.load_image(os.path.join(config['img_dir'], config['image_name']))\n",
    "    transformed_image = transformer.preprocess('data', image)\n",
    "    net.blobs['data'].data[...] = transformed_image\n",
    "    return net, image\n",
    "\n",
    "\n",
    "def extract_detections(detections, det_score_threshold, image_height, image_width):\n",
    "    det_conf = detections[0,0,:,2]\n",
    "    det_x1 = detections[0,0,:,7]\n",
    "    det_y1 = detections[0,0,:,8]\n",
    "    det_x2 = detections[0,0,:,9]\n",
    "    det_y2 = detections[0,0,:,10]\n",
    "    det_x3 = detections[0,0,:,11]\n",
    "    det_y3 = detections[0,0,:,12]\n",
    "    det_x4 = detections[0,0,:,13]\n",
    "    det_y4 = detections[0,0,:,14]\n",
    "    # Get detections with confidence higher than 0.6.\n",
    "    top_indices = [i for i, conf in enumerate(det_conf) if conf >= det_score_threshold]\n",
    "    top_conf = det_conf[top_indices]\n",
    "    top_x1 = det_x1[top_indices]\n",
    "    top_y1 = det_y1[top_indices]\n",
    "    top_x2 = det_x2[top_indices]\n",
    "    top_y2 = det_y2[top_indices]\n",
    "    top_x3 = det_x3[top_indices]\n",
    "    top_y3 = det_y3[top_indices]\n",
    "    top_x4 = det_x4[top_indices]\n",
    "    top_y4 = det_y4[top_indices]\n",
    "\n",
    "    bboxes=[]\n",
    "    for i in xrange(top_conf.shape[0]):\n",
    "        x1 = int(round(top_x1[i] * image_width))\n",
    "        y1 = int(round(top_y1[i] * image_height))\n",
    "        x2 = int(round(top_x2[i] * image_width))\n",
    "        y2 = int(round(top_y2[i] * image_height))\n",
    "        x3 = int(round(top_x3[i] * image_width))\n",
    "        y3 = int(round(top_y3[i] * image_height))\n",
    "        x4 = int(round(top_x4[i] * image_width))\n",
    "        y4 = int(round(top_y4[i] * image_height))\n",
    "        x1 = max(1, min(x1, image_width - 1))\n",
    "        x2 = max(1, min(x2, image_width - 1))\n",
    "        x3 = max(1, min(x3, image_width - 1))\n",
    "        x4 = max(1, min(x4, image_width - 1))\n",
    "        y1 = max(1, min(y1, image_height - 1))\n",
    "        y2 = max(1, min(y2, image_height - 1))\n",
    "        y3 = max(1, min(y3, image_height - 1))\n",
    "        y4 = max(1, min(y4, image_height - 1))\n",
    "        score = top_conf[i]\n",
    "        bbox=[x1,y1,x2,y2,x3,y3,x4,y4,score]\n",
    "        bboxes.append(bbox)\n",
    "    return bboxes\n",
    "\n",
    "def apply_quad_nms(bboxes, overlap_threshold):\n",
    "    dt_lines = sorted(bboxes, key=lambda x:-float(x[8]))\n",
    "    nms_flag = nms(dt_lines, overlap_threshold)\n",
    "    results=[]\n",
    "    for k,dt in enumerate(dt_lines):\n",
    "        if nms_flag[k]:\n",
    "            if dt not in results:\n",
    "                results.append(dt)\n",
    "    return results\n",
    "\n",
    "def save_and_visu(image, results, config):\n",
    "    image_name=config['image_name']\n",
    "    det_save_path=os.path.join(config['det_save_dir'], image_name.split('.')[0]+'.txt')\n",
    "    det_fid = open(det_save_path, 'wt')\n",
    "    if config['visu_detection']:\n",
    "        # visulization\n",
    "        plt.clf()\n",
    "        plt.imshow(image)\n",
    "        currentAxis = plt.gca()\n",
    "    for result in results:\n",
    "        score = result[-1]\n",
    "        x1 = result[0]\n",
    "        y1 = result[1]\n",
    "        x2 = result[2]\n",
    "        y2 = result[3]\n",
    "        x3 = result[4]\n",
    "        y3 = result[5]\n",
    "        x4 = result[6]\n",
    "        y4 = result[7]\n",
    "        result_str=str(x1)+','+str(y1)+','+str(x2)+','+str(y2)+','+str(x3)+','+str(y3)+','+str(x4)+','+str(y4)+','+str(score)+'\\r\\n'\n",
    "        det_fid.write(result_str)\n",
    "        if config['visu_detection']:\n",
    "            quad = np.array([[x1,y1],[x2,y2],[x3,y3],[x4,y4]])\n",
    "            color_quad='r'\n",
    "            currentAxis.add_patch(plt.Polygon(quad, fill=False, edgecolor=color_quad, linewidth=2))\n",
    "\n",
    "    det_fid.close()\n",
    "    if config['visu_detection']:\n",
    "        plt.axis('off')\n",
    "        plt.savefig(config['det_visu_path'], dpi=300)\n",
    "\n",
    "def visu_rec_results(image, rec_save_dir, f_score_threshold):\n",
    "    image_name=config['image_name']\n",
    "    result_file_path = os.path.join(rec_save_dir, image_name.split('.')[0]+'.txt')\n",
    "    rec_result_fid = open(result_file_path, 'r')\n",
    "    plt.clf()\n",
    "    plt.imshow(image)\n",
    "    currentAxis = plt.gca()\n",
    "    for line in rec_result_fid.readlines():\n",
    "        line=line.strip()\n",
    "        x1=int(line.split(',')[0])\n",
    "        y1=int(line.split(',')[1])\n",
    "        x2=int(line.split(',')[2])\n",
    "        y2=int(line.split(',')[3])\n",
    "        x3=int(line.split(',')[4])\n",
    "        y3=int(line.split(',')[5])\n",
    "        x4=int(line.split(',')[6])\n",
    "        y4=int(line.split(',')[7])\n",
    "        det_score=float(line.split(',')[8])\n",
    "        rec_score=float(line.split(',')[10])\n",
    "        rec_str=line.split(',')[9]\n",
    "        f_score = 2*math.exp(det_score)*math.exp(rec_score)/(math.exp(det_score)+math.exp(rec_score))\n",
    "        print(f_score)\n",
    "        if f_score>f_score_threshold:\n",
    "            quad = np.array([[x1,y1],[x2,y2],[x3,y3],[x4,y4]])\n",
    "            color_quad='r'\n",
    "            currentAxis.add_patch(plt.Polygon(quad, fill=False, edgecolor=color_quad, linewidth=2))\n",
    "            currentAxis.text(x1, y1, rec_str, fontsize=5)\n",
    "\n",
    "    rec_result_fid.close()\n",
    "    plt.axis('off')\n",
    "    plt.savefig(config['rec_visu_path'], dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'model_def' : './models/deploy.prototxt',\n",
    "    'model_weights' : './models/model_icdar15.caffemodel',\n",
    "    'img_dir' : './demo_images/',\n",
    "    'image_name' : 'demo.jpg',\n",
    "    'det_visu_path' : './demo_images/demo_det_result.jpg',\n",
    "    'rec_visu_path' : './demo_images/demo_rec_result.jpg',\n",
    "    'det_save_dir' : './demo_images/detection_result/',\n",
    "    'rec_save_dir' : './demo_images/recognition_result/',\n",
    "    'crop_dir' : './demo_images/crops/',\n",
    "    'lexicon_path' : './crnn/data/icdar_generic_lexicon.txt',\n",
    "    'use_lexcion' : True,\n",
    "    'input_height' : 768,\n",
    "    'input_width' : 768,\n",
    "    'overlap_threshold' : 0.2,\n",
    "    'det_score_threshold' : 0.1,\n",
    "    'f_score_threshold' : 0.7,\n",
    "    'visu_detection' : True,\n",
    "    'visu_recognition': False,\n",
    "    'apply_recognition' : False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Could not open file ./models/deploy.prototxt",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c03e6704792a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m#sub_dir='train_images'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mprepare_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mimage_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mdetections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'detection_out'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-b340f67084e9>\u001b[0m in \u001b[0;36mprepare_network\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     17\u001b[0m     net = caffe.Net(config['model_def'],\t # defines the structure of the model\n\u001b[1;32m     18\u001b[0m                 \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# contains the trained weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                 caffe.TEST)     # use test mode (e.g., don't perform dropout)\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mtransformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcaffe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_height'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_width'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Could not open file ./models/deploy.prototxt"
     ]
    }
   ],
   "source": [
    "# detection\n",
    "root_dir=caffe_root+\"data/text/\"\n",
    "for root, dirs, files in os.walk(os.path.join(root_dir,'image_1000/')):\n",
    "    #print(files)\n",
    "    #image path\n",
    "    for file in files:\n",
    "        image_name=file\n",
    "        #label_name=file[0:-4]+'.txt'\n",
    "        image_path=os.path.join(root_dir,'image_1000/',image_name)\n",
    "        #sub_dir='train_images'\n",
    "        config['image_name']=image_name\n",
    "        net, image= prepare_network(config)\n",
    "        image_height, image_width, channels=image.shape\n",
    "        detections = net.forward()['detection_out']\n",
    "        # Parse the outputs.\n",
    "        bboxes = extract_detections(detections, config['det_score_threshold'], image_height, image_width)\n",
    "        # apply non-maximum suppression\n",
    "        results = apply_quad_nms(bboxes, config['overlap_threshold'])\n",
    "        save_and_visu(image, results, config)\n",
    "        print('detection finished')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-ec6206aa1935>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'files' is not defined"
     ]
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
